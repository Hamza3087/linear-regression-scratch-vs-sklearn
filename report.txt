# Linear Regression from Scratch and Comparison with Scikit-Learn

## Abstract
This report presents the implementation and evaluation of a Linear Regression model developed from scratch using Ordinary Least Squares (OLS) and compares its performance with the Scikit-Learn's LinearRegression model. The implementation follows the mathematical foundations of linear regression, utilizing the normal equation method to find the optimal weights. This report includes a detailed explanation of the model architecture, preprocessing steps, training methodology, and comprehensive evaluation metrics.

## 1 Introduction
Linear regression is a fundamental supervised learning algorithm used to predict a continuous target variable based on one or more predictor variables. It establishes a linear relationship between input features and the output, making it an essential baseline model in machine learning. In this study, we implement linear regression from scratch to understand the underlying mathematical principles and compare it with the optimized implementation provided by Scikit-Learn.

## 2 Methodology

### 2.1 Mathematical Foundation
Linear regression models the relationship between a dependent variable y and one or more independent variables X as a linear function:

y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε

where:
- y is the dependent variable
- x₁, x₂, ..., xₙ are the independent variables
- β₀, β₁, β₂, ..., βₙ are the coefficients (parameters) to be estimated
- ε is the error term

The objective is to find the values of β that minimize the sum of squared differences between the observed and predicted values.

### 2.2 Ordinary Least Squares (OLS)
OLS is a method to estimate the parameters of a linear regression model by minimizing the sum of squared residuals. The analytical solution to this optimization problem is given by the normal equation:

β = (X^T X)^(-1) X^T y

where:
- X is the design matrix with a column of ones added for the intercept
- y is the vector of target values
- β is the vector of coefficients (including the intercept term)

### 2.3 Dataset Generation
A synthetic dataset was generated with the following characteristics:
- 1000 samples
- 2 independent variables (x₁, x₂) generated from a standard normal distribution
- True weights: [0.5, -0.7]
- True bias (intercept): 2.0
- Noise level: 0.5 (Gaussian noise with standard deviation of 0.5)

### 2.4 Preprocessing
The dataset underwent the following preprocessing steps:
1. Standardization using StandardScaler from Scikit-Learn to normalize the features
2. Random split into training (80%) and test (20%) sets with a fixed random seed of 42 for reproducibility

### 2.5 Model Implementation
The LinearRegressionScratch class was implemented with the following key methods:
- `fit(X, y)`: Computes weights using the Normal Equation
- `predict(X)`: Predicts values for given inputs
- `mean_squared_error(y_true, y_pred)`: Computes the Mean Squared Error

#### Linear Regression from Scratch Implementation
The implementation follows these steps:
1. Add a column of ones to the feature matrix for the bias term
2. Compute the weights using the Normal Equation
3. Handle potentially singular matrices using the pseudo-inverse when necessary
4. Extract bias and feature weights

#### Scikit-Learn Implementation
For comparison, Scikit-Learn's LinearRegression class was used with default parameters.

## 3 Results and Analysis

### 3.1 Model Parameters
The estimated parameters from both implementations were compared with the true parameters used to generate the data:

| Parameter | True Value | Scratch Model | Scikit-Learn Model |
|-----------|------------|---------------|-------------------|
| Weight 1  | 0.5        | 0.492         | 0.492             |
| Weight 2  | -0.7       | -0.703        | -0.703            |
| Bias      | 2.0        | 1.994         | 1.994             |

Both implementations recovered parameters very close to the true values, indicating successful implementation.

### 3.2 Performance Metrics
The performance of both models was evaluated using Mean Squared Error (MSE) and R² score:

| Metric    | Scratch Model | Scikit-Learn Model |
|-----------|---------------|-------------------|
| MSE       | 0.251         | 0.251             |
| R² Score  | 0.749         | 0.749             |

The nearly identical performance metrics confirm that our from-scratch implementation correctly follows the same mathematical principles as the Scikit-Learn implementation.

### 3.3 Visualization Analysis

#### 3.3.1 Actual vs. Predicted Values
The scatter plots of actual versus predicted values (Fig. 1) for both models show a strong correlation along the ideal prediction line (y = x), indicating good predictive performance. The points cluster tightly around the line, with some dispersion due to the inherent noise in the data.

![Actual vs Predicted Values](actual_vs_predicted.png)
*Fig. 1. Comparison of actual versus predicted values for both models.*

#### 3.3.2 Residual Analysis
The residual plots (Fig. 2) show the difference between the actual and predicted values plotted against the predicted values. For both models, the residuals appear randomly distributed around zero with no discernible pattern, confirming that the linear model assumptions are satisfied.

![Residual Plots](residual_plots.png)
*Fig. 2. Residual plots for both models showing error distribution.*

## 4 Discussion
The implemented linear regression model from scratch achieved performance metrics nearly identical to Scikit-Learn's implementation, validating the correctness of our approach. Both models recovered the true parameters with high accuracy, demonstrating the effectiveness of the OLS method for linear regression.

### 4.1 Implementation Challenges
One potential challenge in implementing OLS is handling singular or near-singular matrices when computing the inverse of X^T X. Our implementation addressed this by using the pseudo-inverse when necessary, ensuring robustness against ill-conditioned data.

### 4.2 Computational Efficiency
While both implementations yield equivalent results, Scikit-Learn's implementation is optimized for computational efficiency and may perform better on larger datasets. Our implementation prioritized clarity and educational value over optimization.

## 5 Conclusion
This study successfully implemented a linear regression model from scratch using the Ordinary Least Squares method and demonstrated its equivalence to the Scikit-Learn implementation. The results confirm that both approaches yield nearly identical parameter estimates and predictive performance.

The implementation provides valuable insights into the mathematical foundations of linear regression and serves as a baseline for understanding more complex regression models. Future work could explore regularization techniques such as Ridge or Lasso regression to handle multicollinearity and prevent overfitting.

## References
1. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Science & Business Media.
2. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830.